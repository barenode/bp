[INTRO]
Dobry den

Me jmeno je Frantisek Hylmar a dnes se zde pokusim obhajit svoji bakalarskou praci Strojové uèení na platformì Apache Spark

V me praci jsem se zameril na aplikaci strojového uèení na velké objemy dat. Konkretne jsem postavil system na doporucovani hudby.

Tato prezentace je rozdelenea do tri casti:

V prvni casti vysvetlim princip doporucovacich systemu zalozenych na faktorizaci matic. 

V druhe casti popisu pouzite ramce pro praci s velkymi objemy dat Apache Spark a Apache Hadoop.

A v posledni treti casti popisu postup a dulezite casti projektu samotneho.


[Doporuèovací systémy]
Pro svuj projekt jsme pouzil pristup zalozen na takzvanem kolaborativnim filtrovani. Pri tomto pristupu se systém snaží identifikovat vazby mezi uzivately a produkty na zaklade jejich predchozi interakce se systemem.
Toto muze byt napriklad shlednuti filmu u online televize nebo zakoupeni knihy u online knihkupectvi.




[Example]
V tomto prikladu mame 4 uzivatele a 4 filmy, kazdy uzivatel hodnotil 2 filmy.  
Z techto hodnoceni sestavime zdrojovou matici pro vypocet, kde je radek pro kazdeho uzivatele a sloupec pro kazdy film.
Realne je takova matice rozsáhlá ale zároveò velice øídká, typicky ménì než 1 % polí má pøiøazenou hodnotu.


Zdrojovou matici faktorizujeme na dve dilci faktorove matice.
Vektory v tìchto faktorových maticích mají nízký poèet rozmìrù. 
Každý tento rozmìr odpovídá skryte vlastnosti modelu.
Hodnoty odpovídají tomu nakolik jsou uživatelé a produkty s touto vlastností spjaty vlastností. 
V tomto pripade mame jeden rozmer pro 



Na Obr. 4 jsou zobrazeny výsledné faktorové matice pro dva faktory. V tomto pøípadì byl
každému faktoru pøiøazen konkrétní význam, jeden pro komedie a druhý pro akèní filmy.
Pøi reálném použití jsou konkrétní významy faktorù skryté a z hlediska algoritmu nejsou
dùležité. Matice s uživatelskými faktory obsahuje pro každého uživatele hodnotu nakolik
je v daných faktorech zastoupen. Stejnì tak i matice s produktovými faktory obsahuje
pro každý produkt hodnotu stejných faktorù. Výsledkem maticového souèinu tìchto
dvou faktorových matic je doplnìná matice obsahující predikce pro všechny uživatele a
produkty.


 

- Java
- x86 arhitektura
- Linux

Jadro Hadoopu je naprogramovane v Jave. Ackoliv je mozne javu provozovat na ruznych operacnich systemech je Hadoop urceny vyhradne pro operacni system Linux a x86 architektutu mikrprocesoru.
Toto je jednak dane tim, ze je zameren na skalovatelnou infrastrukturu v ramci datovych center. V serverovych mikrporocesorech ma x86 architektura 90% podil takze je realne maly zajem na podpore pro jine architektury jako ARM nebo IBM Power Architecture.  
Dalsi z duvpdu je pouziti nativnich knihoven pro optimalizaci nekterych operaci.
Dale take pouziti specifickych instrukcnich rozsireni pro x86.


Jádro tohoto projektu obsahuje tøi základní komponenty.
Distribuovaný souborový systém HDFS, systém na správu prostøedkù poèítaèových
clustrù YARN a výpoèetní systém MapReduce.


[klastr]
Klastr je skupina serveru takzvanych uzlu, poskytujicich jednu nebo vice sluzeb jako jsou napriklad uloziste dat nebo vypocty. V ramaci klastru delime tyto uzly do dvou zakladnich skupin, jedna se o Worker uzly a Master uzly. Worker uzly jsou zodpovedne za realnou praci, na techto pocitacich se ukladaji data a nasledne se na techtro datech provadeji vypocty. Master uzly jsou zodpovedne za koordinaci, udrzovani metadat nebo zotaveni sluzeb v pripade havarii nekterych worker uzlu. V klastru jsou typicky dva nebo tri master uzly a velky pocet worker uzlu. Klastr se sklluje pridavanim vice worker uzlu.
V ramci klustru nekde existuji takzvane gateway nebo edge nody. Na techto serverech nebezi zadne sluzby nezbytne pro funkci klastru ale obsahuji utility a nezbytnou konfiguraci pro pristup ke sluzbam, klusteru.

[HDFS]
Hadoop Distributed File System (HDFS) je distribuovaný souborový systém pro Hadoop, optimalizovaný pro ukládání velkých objemù dat [3]. Pøi ukládání dat, HDFS rozdìlí soubor do blokù konstantní, nakonfigurované délky, standartnì 128 MB. Následnì uloží repliky každého bloku na nakonfigurovaný poèet poèítaèù v clustru, standartnì 3. Replikace blokù dat se provádí jednak z dùvodu zálohy, ale také pro umožnìní paralelních výpoètù nad daty na více poèítaèích v clustru. Každý z poèítaèù v clustru má nainstalovanou HDFS službu DataNode, která je zodpovìdná za ukládání dat na disk a jejich následné naèítání. DataNode služba zná jenom bloky, které má uložené a jejich identifikátory, ale neudržuje informace o tom, které bloky patøí ke kterým uloženým souborùm. Tyto informace jsou udržovány koordinaèní službou NameNode, která udržuje informace o mapování souboru do blokù, dále také udržuje metadata o souborech jako jsou napøíklad pøístupová práva.
HDFS má vysokou prùchodnost. Pokud chce klient uložit soubor, nejprve kontaktuje
NameNode a obdrží list DataNode služeb pro každý blok. Samotný zápis následnì
probíhá mezi klientem a DataNode. Po zapsání bloku na první DataNode tento
automaticky replikuje tento blok na další poèítaèe a neblokuje klienta v dalším zápisu. Stejnì tak pøi naèítání souboru klient komunikuje pøímo s DataNode.
HDFS je tolerantní k chybám. Pokud dojde k havárii disku, poèítaèe nebo dokonce celého racku, NameNode úkoluje jednotlivé DataNode služby, které drží repliky ztracených dat, aby rozkopírovaly ztracené bloky na další poèítaèe v clustru. Tím se zajistí nastavený replikaèní faktor pro každý blok [15].

[YARN]
Yet Another Resource Negotiator (YARN) je centralizovaný clustr manažer pro Hadoop. Každý z poèítaèù v clustru má nainstalovanou YARN službu NodeManager, která komunikuje s øídící službou ResourceManager. Každý NodeManager reportuje službì ResourceManager kolik zdrojù v podobì operaèní pamìti a procesorových jader je dostupných na daném poèítaèi. Zdroje na jednotlivých poèítaèích jsou rozdìlené do logických celkù takzvaných kontejnerù, kde má každý kontejner pøidìlené urèité množství zdrojù (napøíklad 4 procesorová jádra a 8GB RAM). NodeManager je zodpovìdný za vytváøení a monitorovani kontejnerù na daném poèítaèi a jejich ukonèení pokud pøekroèí pøidìlené zdroje. 
Aplikace které potøebují provést výpoèet v rámci clustru nejprve kontaktují službu ResourceManager a zažádají si o jeden kontejner na kterém spustí vlastní koordinaèní proces nazývaný ApplicationMaster (AM). ApplicationMaster si následnì zažádá ResourceManager o potøebné kontejnery na kterých provede výpoèet samotný [15]. 

[SPARK]
Apache Spark [4] je unifikovaný výpoèetní systém pro paralelní zpracování dat na
poèítaèových clustrech. Spark nabízí bohaté API pro datové operace jako je filtrování, spojování (join), seskupování a agregace. Toto API je dostupné pro øadu populárních programovacích jazykù jako jsou Java, Python, C# a R. Spark je aktuálnì nejaktivnìji Vyvíjeným open source projektem v této oblasti s více než tisícem aktivních vývojáøù. Filozofie Sparku je rozdílná od pøedcházejících platforem pro zpracovávání velkých objemù dat jako je napøíklad Hadoop, ten zahrnuje jak výpoèetní systém (MapReduce) tak i úložištì dat (HDFS). Obì tyto èásti jsou spolu úzce provázané a je obtížné provozovat jednu èást bez té druhé. Aèkoliv je možné Spark bez problémù provozovat nad HDFS není na tomto úložném systému nijak závislý a je možné ho používat i spolu s jinými zdroji dat. Jednou z motivací tohoto pøístupu je, že data které je potøeba analyzovat, jsou typicky již uložena v rozdílných formátech v øadì rùzných úložných systémù. Pøesouvání tìchto dat pro analytické úèely mùže být zejména pøi vyšších objemech praktické. Spark je proto postaven tak aby byl pøístup k datùm co nejvíce transparentní.



[Vytvoøení a vyhodnocení modelu]


[Hyper parametry]
[Ohodnocení pøesnosti modelu]


