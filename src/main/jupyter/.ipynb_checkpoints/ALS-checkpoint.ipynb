{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: SPARK_HOME=/usr/hdp/current/spark2-client\n",
      "<pyspark.sql.session.SparkSession object at 0x7f0330651940>\n"
     ]
    }
   ],
   "source": [
    "%env SPARK_HOME=/usr/hdp/current/spark2-client\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import expr, col, column, max, min\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "print(spark)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- User-ID: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      "\n",
      "+------+----+\n",
      "|userId| age|\n",
      "+------+----+\n",
      "|     1|null|\n",
      "|     2|  18|\n",
      "|     3|null|\n",
      "|     4|  17|\n",
      "|     5|null|\n",
      "+------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_raw = spark.read.format(\"csv\")\\\n",
    "    .option(\"sep\",\";\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .load(\"/data/books/BX-Users.csv\")\n",
    "\n",
    "users_raw.printSchema()\n",
    "\n",
    "users = users_raw.select(\\\n",
    "    users_raw['User-ID'].cast('integer').alias('userId'),\\\n",
    "    users_raw['Age'].cast('integer').alias('age'),\\\n",
    ")\n",
    "users.printSchema()\n",
    "users.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- User-ID: string (nullable = true)\n",
      " |-- ISBN: string (nullable = true)\n",
      " |-- Book-Rating: string (nullable = true)\n",
      "\n",
      "1149780\n",
      "root\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- itemId: integer (nullable = true)\n",
      " |-- rating: integer (nullable = true)\n",
      "\n",
      "96397\n"
     ]
    }
   ],
   "source": [
    "ratings_raw = spark.read.format(\"csv\")\\\n",
    "    .option(\"sep\",\";\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .load(\"/data/books/BX-Book-Ratings.csv\")\n",
    "\n",
    "ratings_raw.printSchema()\n",
    "print(ratings_raw.count())\n",
    "\n",
    "ratings = ratings_raw.select(\\\n",
    "    ratings_raw['User-ID'].cast('integer').alias('userId'),\\\n",
    "    ratings_raw['ISBN'].cast('integer').alias('itemId'),\\\n",
    "    ratings_raw['Book-Rating'].cast('integer').alias('rating'),\\\n",
    ")\n",
    "ratings.printSchema()\n",
    "\n",
    "ratings_filtered = ratings.rdd\\\n",
    "    .filter(lambda row: (row['userId'] is not None))\\\n",
    "    .filter(lambda row: (row['itemId'] is not None))\\\n",
    "    .filter(lambda row: (row['rating'] is not None))\\\n",
    "    .toDF()\n",
    "\n",
    "training, test = ratings_filtered.randomSplit([0.8, 0.2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38462\n"
     ]
    }
   ],
   "source": [
    "ratings_grouped = ratings_filtered\\\n",
    "    .groupBy(\"userId\")\\\n",
    "    .count()\\\n",
    "    .rdd\\\n",
    "    .filter(lambda row: (row['count'] > 1))\\\n",
    "    .toDF()\n",
    "print(ratings_grouped.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[userId: bigint, itemId: bigint, rating: bigint, prediction: float]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "\n",
    "als = ALS()\\\n",
    "    .setMaxIter(5)\\\n",
    "    .setRegParam(0.01)\\\n",
    "    .setUserCol(\"userId\")\\\n",
    "    .setItemCol(\"itemId\")\\\n",
    "    .setRatingCol(\"rating\")\n",
    "\n",
    "alsModel = als.fit(training)\n",
    "predictions = alsModel.transform(test)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+----------+\n",
      "|userId| itemId|rating|prediction|\n",
      "+------+-------+------+----------+\n",
      "| 66619| 225755|     0|       NaN|\n",
      "|190079|2232545|     0|       NaN|\n",
      "|189334|2250810|     0| 3.0840244|\n",
      "| 53729|2323338|     0|       NaN|\n",
      "| 63404|2740958|     0|       NaN|\n",
      "+------+-------+------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[userId: bigint, itemId: bigint, rating: bigint, prediction: float]\n"
     ]
    }
   ],
   "source": [
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE = nan\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator = RegressionEvaluator()\\\n",
    "    .setMetricName(\"rmse\")\\\n",
    "    .setLabelCol(\"rating\")\\\n",
    "    .setPredictionCol(\"prediction\")\n",
    "\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"RMSE = %f\" % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
