{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "env: SPARK_HOME=/usr/hdp/current/spark2-client\n",
      "findspark initialized ...\n",
      "pyspark ready ...\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%env SPARK_HOME=/usr/hdp/current/spark2-client\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "print('findspark initialized ...')\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import expr, col, column, max, min\n",
    "\n",
    "spark = SparkSession.builder.appName('mlonspark')\\\n",
    "    .config('spark.executor.instances', '7')\\\n",
    "    .config('spark.jars', '/opt/dev/target/ml-on-spark-1.0.jar')\\\n",
    "    .getOrCreate()\n",
    "\n",
    "print('pyspark ready ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- artistId: integer (nullable = true)\n",
      " |-- listenCount: float (nullable = true)\n",
      " |-- scaled-by-user: float (nullable = true)\n",
      " |-- scaled-by-artist: float (nullable = true)\n",
      "\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "train = spark.read.load(\"/data/lastfm-dataset-360K/coo-data-train.parquet\")\n",
    "train.printSchema()\n",
    "print(train.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "from mlonspark.alternating_least_square import AlternatingLeastSquare\n",
    "from pyspark.ml.recommendation import ALS\n",
    "\n",
    "als = ALS()\\\n",
    "    .setUserCol(\"userId\")\\\n",
    "    .setItemCol(\"artistId\")\\\n",
    "    .setRatingCol(\"scaled-by-artist\")\\\n",
    "    .setAlpha(1.0)\\\n",
    "    .setNumUserBlocks(7)\\\n",
    "    .setNumItemBlocks(7)\\\n",
    "    .setMaxIter(10)\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALS_48c890458f46ab43dd68\n"
     ]
    }
   ],
   "source": [
    "model = als.fit(train)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "test = spark.read.load(\"/data/lastfm-dataset-360K/coo-data-test.parquet\")\n",
    "print(test.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- artistId: integer (nullable = true)\n",
      " |-- listenCount: float (nullable = true)\n",
      " |-- scaled-by-user: float (nullable = true)\n",
      " |-- scaled-by-artist: float (nullable = true)\n",
      " |-- prediction: float (nullable = false)\n",
      "\n",
      "+------+--------+-----------+--------------+----------------+----------+\n",
      "|userId|artistId|listenCount|scaled-by-user|scaled-by-artist|prediction|\n",
      "+------+--------+-----------+--------------+----------------+----------+\n",
      "| 38304|     148|       23.0|     1.3435115|       1.1165468| 1.4010674|\n",
      "|153214|     148|      166.0|      1.438134|       2.9683454| 3.9653459|\n",
      "| 54065|     148|      229.0|     3.4793928|       3.7841725| 1.9848043|\n",
      "|144880|     148|      145.0|     1.1409639|        2.696403|  2.299455|\n",
      "|263510|     148|       59.0|     1.1044855|       1.5827338| 1.6094072|\n",
      "| 32705|     148|       77.0|     2.7674417|       1.8158274| 1.4124448|\n",
      "|177946|     148|       25.0|     1.2352941|        1.142446| 1.4811388|\n",
      "| 72977|     463|      131.0|     1.9130435|        3.374372| 1.4796325|\n",
      "|191154|     463|      424.0|     1.0429919|            10.0| 1.9845779|\n",
      "|326012|     463|      350.0|     1.3878808|        8.326633| 2.3288236|\n",
      "+------+--------+-----------+--------------+----------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(test)\n",
    "predictions.printSchema()\n",
    "predictions.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnan\n",
    "nan = predictions.where(isnan(col(\"listenCount\")))  \n",
    "nan.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE = 641.327225\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import isnan\n",
    "\n",
    "evaluator = RegressionEvaluator()\\\n",
    "    .setMetricName(\"rmse\")\\\n",
    "    .setLabelCol(\"listenCount\")\\\n",
    "    .setPredictionCol(\"prediction\")\n",
    "\n",
    "predictionsFiltered = predictions.where(~isnan(col(\"prediction\")))                                \n",
    "rmse = evaluator.evaluate(predictionsFiltered)\n",
    "\n",
    "print(\"RMSE = %f\" % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#histogram = train.select(train[\"listenCount\"]).rdd.map(lambda x : x[0]).histogram(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "#\n",
    "#import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "#\n",
    "#def create_hist(rdd_histogram_data):\n",
    "#    \"\"\"Given an RDD.histogram, plot a pyplot histogram\"\"\"\n",
    "#    heights = np.array(rdd_histogram_data[1])\n",
    "#    full_bins = rdd_histogram_data[0]\n",
    "#    mid_point_bins = full_bins[:-1]\n",
    "#    widths = [abs(i - j) for i, j in zip(full_bins[:-1], full_bins[1:])]\n",
    "#    bar = plt.bar(mid_point_bins, heights, width=widths, color='b')\n",
    "#    return bar\n",
    "#\n",
    "#create_hist(histogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import TrainValidationSplit, ParamGridBuilder\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator = RegressionEvaluator()\\\n",
    "    .setMetricName(\"rmse\")\\\n",
    "    .setLabelCol(\"scaled-by-artist\")\\\n",
    "    .setPredictionCol(\"prediction\")\n",
    "\n",
    "pipeline = Pipeline().setStages([als])\n",
    "params = ParamGridBuilder()\\\n",
    "    .addGrid(als.maxIter, range(2, 20, 2))\\\n",
    "    .build()    \n",
    "\n",
    "cv = TrainValidationSplit()\\\n",
    "    .setTrainRatio(0.75)\\\n",
    "    .setEstimator(als)\\\n",
    "    .setEstimatorParamMaps(params)\\\n",
    "    .setEvaluator(evaluator)\n",
    "\n",
    "model = cv.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[nan, nan, nan, nan, nan, nan, nan, nan, nan]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.validationMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import TrainValidationSplit, ParamGridBuilder\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "\n",
    "pipeline = Pipeline().setStages([als])\n",
    "params = ParamGridBuilder()\\\n",
    "    .addGrid(als.rank, range(2, 20, 2))\\\n",
    "    .build()    \n",
    "\n",
    "cv = TrainValidationSplit()\\\n",
    "    .setTrainRatio(0.75)\\\n",
    "    .setEstimator(als)\\\n",
    "    .setEstimatorParamMaps(params)\\\n",
    "    .setEvaluator(evaluator)\n",
    "\n",
    "rankModel = cv.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[nan, nan, nan, nan, nan, nan, nan, nan, nan]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rankModel.validationMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALS_4fa9a871c38887e12f3b\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "from mlonspark.alternating_least_square import AlternatingLeastSquare\n",
    "from pyspark.ml.recommendation import ALS\n",
    "\n",
    "#    \n",
    "als = ALS()\\\n",
    "    .setUserCol(\"userId\")\\\n",
    "    .setItemCol(\"artistId\")\\\n",
    "    .setRatingCol(\"scaled-by-artist\")\\\n",
    "    .setNumUserBlocks(7)\\\n",
    "    .setNumItemBlocks(7)\\\n",
    "    .setMaxIter(10)\\\n",
    "    .setRank(15)\\\n",
    "    .setImplicitPrefs(False)\\\n",
    "\n",
    "model = als.fit(train)\n",
    "print(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE = 0.942539\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import isnan\n",
    "\n",
    "predictions = model.transform(test)\n",
    "\n",
    "evaluator = RegressionEvaluator()\\\n",
    "    .setMetricName(\"rmse\")\\\n",
    "    .setLabelCol(\"scaled-by-artist\")\\\n",
    "    .setPredictionCol(\"prediction\")\n",
    "\n",
    "predictionsFiltered = predictions.where(~isnan(col(\"prediction\")))                                \n",
    "rmse = evaluator.evaluate(predictionsFiltered)\n",
    "\n",
    "print(\"RMSE = %f\" % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
