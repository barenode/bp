
==  Apache Spark

Apache Spark cite:[SPARK] je unifikovaný výpočetní systém pro paralelní zpracování dat na počítačových clustrech. Spark nabízí bohaté API pro datové operace jako je filtrování, spojování (join), seskupování a agregace. Toto API je dostupné pro řadu populárních programovacích jazyků jako jsou Java, Python, C# a R. Spark je aktuálně nejaktivněji vyvíjeným open source projektem v této oblasti s více než tisícem aktivních vývojářů.

Filozofie Sparku je rozdílná od předcházejících platforem pro zpracovávání velkých objemů dat jako je například Hadoop, ten zahrnuje jak výpočetní systém (MapReduce) tak i úložiště dat (HDFS). Obě tyto části jsou spolu úzce provázané a je obtížné provozovat jednu část bez té druhé. Ačkoliv je možné Spark bez problémů provozovat nad HDFS není na tomto úložném systému nijak závislý a je možné ho používat i spolu s jinými zdroji dat. Jednou z motivací tohoto přístupu je, že data které je potřeba analyzovat, jsou typicky již uložena v rozdílných formátech v řadě různých úložných systémů. Přesouvání těchto dat pro analytické účely může být zejména při vyšších objemech nepraktické. Spark je proto postaven tak aby byl přístup k datům co nejvíce transparentní.

Klíčovou vlastností Sparku je, že samotné provedení sekvence datových operací je nejprve optimalizováno. Tato optimalizace zajistí co možná nejefektivnější řetězec zpracování s využítím operační paměti pro mezivýsledky. To je velkou výhodou v rychlosti výpočtu oproti MapReduce kde se v každém kroku výsledky perzistují a je nutné je v následujícím kroku řetězce znovu načíst.    

V diagramu <<spark-structure>> je zobrazena základní struktura Sparku. Základem je nízko-úrovňové API pro práci s datasety, RDD (Resilient Distributed Dataset) volně přeloženo jako pružný distribuovaný dataset. Dalším patrem je strukturované API, přidané v druhé generaci Sparku. Na RDD a strukturovaném API je postavena řada specifických modulů, které jsou součástí standartní distribuce Sparku. Mezi tyto moduly patří:

* MLlib pro podporu strojové učení. 
* Structured Spark Streaming pro podporu datových proudů.
* GraphX pro podporu analýzy grafů.     

[[spark-structure]]
image::spark-structure.png[title="Struktura Sparku cite:[SDG]", pdfwidth="75%"]


=== Spark API

Pro vytváření vlastních aplikací existují ve Sparku dvě programové rozhraní (API). Jedná se základní RDD API a jeho nadstavbu, strukturované API viz. <<spark-structure>>.  

==== RDD

RDD (Resilient Distributed Dataset) je základní konstrukt Spark API. Představuje neměnnou kolekci záznamů, rozdělenou do částí, které mohou být nezávisle paralelně zpracovány. Tyto části, takzvané partitions, jsou typicky rozložené na více uzlů v rámci výpočetního clustru. Jednotlivé záznamy jsou klasické Java, Python nebo Scala objekty. Při operaci nad daným RDD je každé partititon přiřazen právě jeden výkonný proces. RDD API dále disponuje řadou operací pro manipulaci s daty, které se dělí do dvou základních skupin:

Transformace::
    Ve Sparku jsou zdrojová data typicky neměnná. Pokud chceme provést úpravy nad RDD, definujeme jednu nebo více takzvaných transformací. Tyto transformace instruují Spark jak má změnit zdrojová data. Důležitý je fakt, že samotná transformace pouze definuje jak se mají data změnit, transformace samotná se ale instantně neiniciuje. Komplexní operace nad daty většinou zahrnují celý řetězec atomických transformací. Spark počká na akci která si vyžádá transformovaný RDD, z řetězce transformací následně automaticky vyhodnotí optimální plán provedení a teprve potom transformuje zdrojový RDD. Transformace se dále dělí na dva základní typy, na úzké a široké. Úzké transformace se aplikují na jednu partition nezávisle na ostatních. Výsledkem je právě jedna cílová partition, typicky uložená na stejném počítači v rámci clustru. Úzké transformace se tedy nechají jednoduše paralelizovat. Při široké transformaci je transformovaná partition závislá na ostatních partition, typicky uložených na ostatních počítačích. Je tedy nutné provést takzvané přeskládání (shuffle) a v rámci clustrové sítě přenést potřebná data.           

Akce::
    Akce jsou metody RDD API, které spouští samotný výpočet, při kterém se aplikují definované transformace na zdrojový RDD. 

 
V následujícím jednoduchém příkladu spočítáme počet lichých a sudých čísel v datasetu:

[source, scala, numbered]
----
include::/opt/dev/src/test/scala/mlonspark/SCSuite.scala[tags=spark-example-1]
----
  
Pro tento příklad vyhodnotí Spark následující plán zpracování, kde se čísla na konci řádků odkazují na řádky ve uvedeném zdrojovém kódu:

---- 
(2) MapPartitionsRDD[3] at mapValues at 4 <4>
 |  ShuffledRDD[2] at groupByKey at 3 <3>
 +-(2) MapPartitionsRDD[1] at map at 2 <2>
    |  ParallelCollectionRDD[0] at parallelize at 1 <1>
----
<1> Na řádku 1 nejprve vytvoříme zdrojový dataset. Ten v tomto případě inicializujeme přímo v kódu na hodnoty v intervalu <1, 9> ačkoliv typickým použitím je načíst zdrojová data z perzistentního úložiště. 

<2> Na řádku 2. použijeme RDD metodu *map*, tato metoda má jako argument funkci, která trasnsformuje jednu z hodnot zdrojového RDD. V plánu zpracování je jako výsledek tohoto kroku MapPartitionsRDD, to znamená, že se transformace provede  lokálně na úrovni jednotlivých partition. Jako výsledek transformace je RDD dvojic kde je první hodnota zbytek po celočíselném dělení dvěmi. Pokud jsou v RDD dvojice, Spark automaticky interpretuje první hodnotu jako klíč.  

<3> Na řádku 3. pomocí RDD metody *groupByKey* seskupíme hodnoty se stejným klíčem. Výsledkem tedy bude RDD dvojice s příslušným klíčem, kde je jako hodnota kolekce všech hodnot, které mají ve zdrojovém RDD daný klíč. V plánu zpracování je jako výsledek tohoto kroku ShuffledRDD, to znamená, že je třeba provést přeskládání v rámci výpočetního clustru a seskupit hodnoty se stejným klíčem.

<4> Na řádku č. pomocí RDD metody *mapValues* transformujeme pouze hodnoty, klíč ve dvojici zůstává nezměněn. Zde je jako hodnota kolekce čísel odpovídající danému zbytku po celočíselném dělení. V rámci této operace tuto kolekci převedeme na celé číslo odpovídající počtu členů této kolekce. V plánu zpracovaní je jako výsledek tohoto kroku MapPartitionsRDD, transformace se tedy také provede lokálně na úrovni jednotlivých partition. 

Výsledkem je 5 lichých hodnot (klíč 1) a 4 sudé hodnoty (klíč 0):  
  
----
(1,5)
(0,4)
----

==== Strukturované API [[spark-struct-api]]

Do druhé generace přidali autoři Sparku takzvané strukturované API. Základním konstruktem tohoto API je DataFrame. DataFrame reprezentuje tabulku složenou z řádků a sloupců, tato tabulka se ale, stejně jako RDD, může rozkládat na mnoha počítačích v rámci výpočetního clustru. DataFrame také jako RDD používá pro operace s daty akce a transformace. Strukturované API by mělo být preferovaným způsobem používání Sparku, je uživatelsky přívětivější, vysoce optimalizované a odstiňuje uživatele od náročnějších detailů. Nicméně pokud je třeba mít přímou kontrolu nad tím, jak jsou data fyzicky uložena v rámci clustru, je nutné použít RDD API.
Stejný koncept jako DataFrame, omezený na jediný počítač, používají API jako Python Pandas nebo R DataFrames. Toto usnadňuje používání Sparku uživatelům se znalostmi těchto nástrojů, například jako doplňující nástroj pro práci s velkými objemy dat. 


=== Spark Aplikace

Spark aplikace se skládá z řídícího procesu a sady výkonných procesů. Řídící proces je zodpovědný za analýzu a distribuci jednotlivých úkolů výkonným procesům. Výkonné procesy jsou zodpovědné za zpracování úkolů, které jim přiřadí řídící proces a za reportování stavu tohoto úkolu zpět řídícímu procesu. Clustr počítačů, které Spark využívá pro vykonání dané aplikace je řízený clustr manažerem. Manažerský proces řídí přístup k prostředkům clustru a přiřazuje jeho zdroje jednotlivým aplikacím. V rámci jednoho clustru tedy může být spuštěno více Spark aplikací zároveň. Spark není závislý na jednom konkrétním clustr manažeru, v době psaní práce podporoval Hadoop YARN, Apache Mesos a také vlastní manažer, omezený pouze na jediný počítač. Vývojář Spark aplikace je odstíněný od toho na jaké úkoly bude aplikace rozdělena nebo na kterých počítačích v rámci clustru budou tyto úkoly vykonány. O vše se transparentně postará Spark spolu s použitým clustr manažerem. Průběh Spark aplikace lze rozdělit do několika fází:                                 


Inicializace::
    Aplikace samotná je typicky java knihovna obsahující spustitelnou třídu. Způsobů jak spustit aplikaci je více, základní z nich je použít utilitu spark-submit z příkazové řádky. V této chvíli spouštíme proces na klientském počítači, tento proces kontaktuje příslušný clustr manažer a zažádá si o prostředky pro řídící proces. Clustr manažer umístí řídící proces samotný na některý z dostupných počítačů clustru. Proces spuštěný z příkazové řádky na klientském počítači je ukončen a aplikace je spustěna. Průběžný stav aplikace je možné sledovat pomocí dodatečných dotazů na příšlušný clustr manažer. V této chvíli je tedy řídící proces umístěn na některém počítači v clustru, řídící proces následně aktivuje uživatelský kód (spustitelná třída v rámci odeslaného JARu). Tento kód musí obsahovat inicializaci třídy *SparkSession*. SparkSession následně komunikuje s clustr manažerem a vyžádá si spuštění jednotlivých výkonných procesů. Po inicializaci a startu těchto výkonných procesů odešle clustr manažer relevantní informace o jejich umístění zpět řídícímu procesu.

Vykonání::
    V této chvíli máme inicializovaný Spark clustr složený z jednoho řídícího procesu a sady výkonných procesů. Řádící proces přiděluje jednotlivé úkoly výkonným procesům. Výkonné procesy komunikují mezi sebou, vykonávají přidělené úkoly a vyměňují si potřebná data. Po dokončení přiděleného úkolu reportují výsledný status zpět řídícímu procesu.

Dokončení::
    Po dokončení běhu aplikace, je řídící proces ukončen s výsledným stavem. Clustr manažer následně ukončí všechny přidělené výkonné procesy. V této chvíli je možné zjistit konečný status aplikace dotazem na příslušný clustr manažer.


