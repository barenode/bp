
=== Instalace Apache Spark

Pro vývoj a testování vyvíjené aplikace je třeba nainstalovat některou z distribucí Sparku. Nejjednodušší na instalaci je používat Spark v takzvaném stadalone módu. Tato distribuce používá vlastní clustr manažer, určený pro použití na jediném počítači. V tomto módu se typicky používá lokální souborový systém a výpočet je distribuován výhradně na jádra jediného procesoru. Vzhledem k tomu, že se tato práce zabýva distribuovanými výpočty, byl by tento mód pro vývoj aplikace příliš zjednodušující. Vhodnější je zvolit některou z forem clustrových distribucí. Zde se nabízí použít Spark v rámci některé Hadoop distribuce, toto je také nejběžnější forma nasazení Sparku v rámci organizací zabývajících se zpracováním velkých objemů dat. Největší společnosti nabízející distribuce Hadoopu jsou Cloudera a Hortnonworks. Ačkoliv při produkčním použití jsou jejich produkty licencovány per jádro, obě dvě společnosti nabízí volně dostupné repozitáře pro hlavní distribuce linuxu. Dále také nabízejí nástroje pro usnadnění instalace jednotlivých služeb. Typicky takový nástroj automaticky nainstaluje kompletní Hadoop clustr dle zvolené topologie. Další variantou je použít některou z variant Hadoopu, které jsou nabízeny jako služba v kloudových systémech. Největší hráči Microsoft a Amazon aktuálně nabízejí Hadooop resp. Spark v rámci jejich portfolia služeb. U Microsoftu se služba jmenuje Azure HDInsight a u Amazonu Amazon EMR. Obrovskou výhodou kloudového řešení je, že se obejdeme bez nutnosti instalovat jednotlivé komponenty na jednotlivé počítače v clustru. Celý Hadoop ekosystém zahrnuje velké množství na sobě závislých služeb, které je nutné nainstalovat podle zvolené topologie na jednotlivé počítače. Dále je nutné jednotlivé služby nakonfigurovat tak aby mohly vzájemně spolupracovat. I v případě, že chceme používat Spark samotný, je třeba na celém clustru nainstalovat distribuovaný souborový systém HDFS a clustru manažer YARN. Toto vše udělá kloud za nás a celý systém je možný používat v řádech desítek minut. Nevýhodou je ovšem v tomto případě cena. I když zvolíme minimální variantu, nedostaneme se pod desítky Euro měsíčne což není pro akademické použití příliš vhodné.
Po zvážení všech důvodů se pro testování nasazení aplikace použil virtuální clustr s distribucí Hadoopu od společnosti Hortonworks. Tato společnost nazývá svůj produkt HDP (Hortonworks Data Platform). 

[[hdp-architecture-diagram]]
image::hdp-architecture-diagram.png[title="Hortonworks Data Platform", pdfwidth="100%"]

Na <<hdp-architecture-diagram>> je zobrazen ekosystém služeb HDP. Tento ekosystém zahrnuje mimo základních prvků Hadoopu jako jsou Spark, HDFS, YARN mnoho dalších služeb jako například NoSQL databáze HBASE, framework pro podporu deep lerningu TensorFlow atd. Pro testování aplikace ale nainstalujeme pouze bezpodmínečně nutné služby. Pro usnadnění správy clustru je v HDP použita aplikace Apache Ambari viz. <<hdp-architecture-diagram>> sekce Operations & Orchestration. Tato aplikace má takzvanou master-slave architekturu. Na všech  počítačích v clustru je nainstalována klientská služba. Ta má ze úkol instalovat vybrané balíky na daný počítač, dále tato služba reportuje systémové informace pro účely monitoringu. Na jeden vybraný uzel v klastru je nainstalována serverová (master) služba aplikace. Ta dle potřeby instruuje jednotlivé připojené klienty a sbírá data pro monitoring, tato služba dále disponuje REST a  webovým rozhraním. Přes webové rozhraní je možné jednoduše spravovat jenotlivé služby v rámci celého clustru. Přes REST rozhraní je možné nainstalovat kompletně celý clustr dle zadané definice. Tyto definice jsou v JSON formátu, takzvané Ambari Blueprints.    

====  Instalace HDP

Jako virtualizační platforma pro hostitelský počítač byl vybrán VirtualBox od společnosti Oracle. Tato platforma je volně dostupná a podporuje většinu operačních systémů. Pro usnadnění práci s virtuálními operačnímy systémy je inicializace řízena utilitou Vagrant. Vagrant umožnuje jednoduše definovat jaké virtualní stroje je potřeba vytvořit, jaké mají mít paramtery, sdílené adresáře atd. Samotná práce s VirtualBoxem se tím výrazně zjednodušuje. Filozofie za tímto nástrojem je taková, že je tyto prostředí potřeba vytyvářet opakovaně a plně automaticky, tak aby vývojáři nebyli nuceni neustále opakovat stejné konfigurační úkony. Úkony jako instalace databází, webových a aplikačních serverů jsou místo toho definovány ve skriptech a Vagrant je při vytváření virtuálních strujů automaticky aplikuje. Vagrant podporuje nejznámnější konfigurační rámce jako jsou Puppet nebo Chief, podporuje ale také obyčejné shell skripty.  
Jako operační systém byla vybrána distribuce linuxu Centos 7, pro kterou společnost Hortonworks spravuje volně dostupné Yum repozitáře. Pro instalalaci testovacího prostředí byly použity následující repozitáře a verze:

[[hdp-yum-repo]]
[title="HDP Yum repozitáře pro Centos 7", cols="3,^2,10", options="header"]
|=========================================================
|Jméno |Verze |URL
|Ambari|2.7.4.0|http://public-repo-1.hortonworks.com/ambari/centos7/2.x/updates/2.7.4.0/ambari.repo
|HDP|3.1.4.0|http://public-repo-1.hortonworks.com/HDP/centos7/3.x/updates/3.1.4.0/hdp.repo
|HDP-UTILS|1.1.0.22|http://public-repo-1.hortonworks.com/HDP-UTILS-1.1.0.22/repos/centos7
|=========================================================

Pro zrychlení instalace a omezení množství stahovaných dat při opakované instalaci je výhodné použít lokalní repozitáře. Jednotlivé Yum repositáře stáhneme v komprimované podobě, tyto lokální repozitáře následně uložíme na hostitelském stroji a přes sdílené adresáře učiníme dostupné všem virtualním strojům. Dále je nutné stáhnout některé knihovny, které jsou sice zadarmo ale vyžadují registraci u společnosti Oracle. Jedná se o distribuci Java SDK a dále java konektor pro databázi MySQL. Konečná adresářová struktura v souborovém systém hostitelského počítače by tedy měla vypadat následovně:

----
/yum
    /repo
        /ambari <1>
        /hdp    
            /HDP         <2>      
            /HDP-UTILS   <3>             
        /common
         |- jdk-8u231-linux-x64.tar.gz  <4>   
         |- mysql-connector-java.jar    <5> 
----  
<1> Obsah Yum repozitáře Ambari. Viz <<hdp-yum-repo>>
<2> Obsah Yum repozitáře HDP. Viz <<hdp-yum-repo>>
<3> Obsah Yum repozitáře HDP-UTILS. Viz <<hdp-yum-repo>>
<4> Distribuce Oracle Java 8 SDK (build 231).
<5> Java konkektor pro databázi MySQL.

Clustr je složen ze čtyř virtuálních počítačů viz. <<cluster-topolgy>>. Počítač c7201 obsahuje všechny řídící procesy pro HDFS a Yarn, dále také obsahuje utility pro volání služeb klusteru. V produkčních clustrech jsou typicky tyto dvě role rozděleny na rozdílných počítačích. Dále clustr obsahuje tři naprosto identické počítače c7202, c7203, c7204. Tyto počítače budou zpracovávat jednotlivé úlohy distribuovaných výpočtů Sparku. Každý z těchto uzlů zároveň slouži jako datový uzel pro HDFS. Počet pracovních uzlů (3) odpovídá výchozímu replikačnímu faktoru HDFS. Reálně to tedy znamená že jsou všechny data uloženy na všech třech pracovních uzech.          

[[cluster-topolgy]]
[title="Topologie klastru", options="header"]
|=========================================================
|Jméno |IP adresa |RAM   |Počet jader| Role
|c7201|192.168.72.101|6GB|3|Master, Edge
|c7202|192.168.72.102|6GB|2|Worker
|c7203|192.168.72.103|6GB|2|Worker
|c7204|192.168.72.104|6GB|2|Worker
|=========================================================

Soubor Vagrantfile je uložen v podadresáři vagrant v kořenovém adresáři projektu. V tomto souboru jsou nadefinovány jednotlivé virtuální stroje a jejich parametry. 

[source, ruby, numbered]
----
include::../../../vagrant/Vagrantfile[tags=box-def]
----

Atribut config.vm.box obsahuje definici boxu z kterého budou vytvořeny všechny nadefinované virtuální stroje. Tyto boxy jsou uloženy ve veřejném repositáři nástroje Vagrant. Při vytvoření prvního virtuálního počítače se tento box automaticky stáhne z příslušného repozitáře a následně se používá lokálně uložena kopie.

[source, ruby, numbered]
----
include::../../../vagrant/Vagrantfile[tags=synced-folders]
----

Atributy config.vm.synced_folder obsahují definice sdílených adresářů. Dle této definice se adresáře ze souborového systému automaticky připojí na souborový systém virtuálních strojů. První definice připoji kořenový adresář projektu do adresáře /opt/dev virtuálních strojů. Tato definice požívá relativní cestu vzhledem k Vagrantfile souboru. Druhá definice pripojí lokalní Yum repozitař do adresáře /var/www/html virtuálních strojů. Soubory uložené v tomto adresáři jsou následně dostupné přes HTTP protokol a příslušné URL mohou být použité k definici jednotlivých Yum repozitářů na virtuálních počítačích.
 
[source, ruby, numbered]
----
include::../../../vagrant/Vagrantfile[tags=vm-settings]
----

Sekce config.vm.provider obsahuje konfigurace specifické pro virtualizační platformu. V tomto případě instruujeme VirtualBox kolik operační paměti a jader procesoru má dát k dispozici jednotlivým virtuálním počítačům.  

[source, ruby, numbered]
----
include::../../../vagrant/Vagrantfile[tags=c7202]             
----

Sekce config.vm.define obsahuje definici virtuálního stroje c7202 viz <<cluster-topolgy>>. Pro tento virtální počítač defujeme pouze specifickou IP adresu a jméno hosta. Všechny ostatní parametry jsou nadefinovány globálně, společně pro všechny virtuálním počítače. Incializaci clustru spustíme z příkazové řádky relativně k deskriptou Vagrantfile:

----
  > vagrant up
----