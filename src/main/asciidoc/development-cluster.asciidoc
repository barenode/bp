
=== Instalace Apache Spark

Pro vývoj a testování vyvíjené aplikace je třeba nainstalovat některou z distribucí Sparku. Nejjednodušší na instalaci je používat Spark v takzvaném stadalone módu. Tato distribuce používá vlastní manažer klusteru, určený pro použití na jediném počítači. V tomto módu se typicky používá lokální souborový systém a výpočet je distribuován výhradně na jádra jediného procesoru. Vzhledem k tomu, že se tato práce zabýva výpočty distribuovanými přes klastr více počítačů, byl by tento mód pro vývoj aplikace příliš zjednodušující. Vhodnější by bylo zvolit některou z forem klusterových distrubucí. Zde se nabízí použít Spark v rámci některé Hadoop distribuce, toto je také nejběžnější forma nasazení Sparku rámci organizací zabývajíích se zpracováním velkých objemů dat. Největší společnosti nabízející distribuce Hadoopu jsou Cloudera a Hortnonworks. Obě dvě společnosti nabízí volně dostupné repozitáře pro hlavní distribuce linuxu. Dale také nabízejí nástroje pro usnadnění instalace jednotlivých služeb. Typicky takový nástroj vše automaticky nainstaluje dle zvolené topologie. Dále se nabízí použít nějkterou z variant Hadoopu, které jsou nabízeny jako služba v kloudových systémech. Největší hráči Microsoft a Amazon aktuálně nabízejí Hadooop resp. Spark v rámci jejich portfolia služeb. U Microsoftu se služba jmenuje Azure HDInsight u Amazonu Amazon EMR. Obrovskou výhodou kloudového řešení je, že se obejdeme bez nutnosti instalovat jednotlivé komponenty na jednotlivé počítače v klusteru. Celý Hadoop ekosystém zahrnuje vélké množství na sobě závislých služeb, které je nutné nainstalovat podle zvolené topologie na jednotlivé uzly. Dále je nutné jednotlivé služby nakonfigurovat tak aby mohly vzájemně spolupracovat. I v případě, že chceme používat Spark samotný, je třeba na celém klusteru nainstalovat distribuovaný souborový systém HDFS a manažer klusteru YARN. Toto vše udělá kloud za nás a celý systém je možný používat v řádech desítek minut. Nevýhodou je v tomto případě cena. I když zvolíme minimální variantu, nedostaneme se pod desítky Euro měsíčne což je pro akademické použití nepříliš vhodné.
Po zvážení všech důvodů se pro testování aplikace použil virtuální kluster s distribucí Hadoopu od společnosti Hortonworks. Tato společnost nazývá svůj produkt HDP (Hortonworks Data Platform). 

[[hdp-architecture-diagram]]
image::hdp-architecture-diagram.png[title="Hortonworks Data Platform", pdfwidth="100%"]

Na <<hdp-architecture-diagram>> je zobrazen ekosystém služeb HDP. Tento ekosystém zahrnuje mimo základních prvků Hadoopu jako jsou Spark, HDFS, YARN mnoho dalších služeb jako jsou například NoSQL databáze HBASE, framework pro podporu deep lerningu TensorFlow atd. Pro testování aplikace nainstalujeme pouze bezpodmínečně nutné služby. Pro usnadnění správy klusteru je v HDP použita aplikace Apache Ambari viz. <<hdp-architecture-diagram>> sekce Operations & Orchestration. Tato aplikace má takzvanou master-slave architekturu. Na všech  uzlech je nainstalována klientská služba. Ta má ze úkol instalovat vybrané balíky na daný uzel, dále klientská část reportuje systémové informace pro účely monitoringu. Na jeden vybraný uzel v klastru je nainstalována serverová část aplikace. Ta podle potřeby instruuje jednotlivé připojené klienty, sbírá data pro monitoring, atd. Serverová část dále disponuje REST a  webovým rozhraním. Přes webové rozhraní je možné jednoduše spravovat jenotlivé služby v rámci celého klastru. Přes REST rozhraní je možné nainstalovat kompletně celý klastr podle zadané definice. Tyto definice jsou v JSON formátu, takzvané Ambari Blueprints.    

====  Instalace HDP

Jako virtualizační platforma pro hostitelský počítač byl vybrán VirtualBox od společnosti Oracle. Tato platforma je volně dostupná a podporuje většinu operačních systémů. Pro usnadnění práci s virtuálními operačnímy systémy je inicializace řízena utilitou Vagrant. Vagrant umožnuje jednoduše definovat jaké virtualní stroje je potřeba vytvořit, jaké mají mít paramtery, sdílené adresáře atd. Samotná práce s VirtualBoxem se tím výrazně zjednodušuje. Filozofie za tímto nástrojem je taková, že je tyto prostředí potreba vytyvářet opakovaně a plně automaticky tak aby vývojáři nebyli nuceni neustále opakovat stejné konfigurační úkony. Úkony jako instalace databází, webových a aplikačních serverů jsou místo toho definovány ve skriptech a Vagrant je pri vytváření virtuálních strujů automaticky aplikuje. Vagrant podporuje nejznámnější konfigurační rámce jako jsou Puppet nebo Chief, podporuje také obyčejné shell skripty.  
Jako operační systém byla vybrána volně dostupná distribuce linuxu Centos 7. Pro Centos 7 společnost Hortonworks spravuje veřejné Yum repozitáře. Pro instalalaci testovacího prostředí aplikace byly použity následující repozitáře a verze:

[[hdp-yum-repo]]
[title="HDP Yum repozitáře pro Centos 7", cols="3,^2,10", options="header"]
|=========================================================
|Jméno |Verze |URL
|Ambari|2.7.4.0|http://public-repo-1.hortonworks.com/ambari/centos7/2.x/updates/2.7.4.0/ambari.repo
|HDP|3.1.4.0|http://public-repo-1.hortonworks.com/HDP/centos7/3.x/updates/3.1.4.0/hdp.repo
|HDP-UTILS|1.1.0.22|http://public-repo-1.hortonworks.com/HDP-UTILS-1.1.0.22/repos/centos7
|=========================================================

Pro zrychlení instalace a omezení množství stahovaných dat při opakované instalaci je výhodné použít lokalní repozitáře. Jednotlivé Yum repositáře stáhneme v komprimované podobě. Tyto lokální repozitáře následně uložíme na hostitelském stroji a přes sdílené adresáře učiníme dostupné všem virtualním strojům. Dále je nutné stáhnout některé knihovny, které jsou sice zadarmo ale vyžadují registraci u společnosti Oracle. Jedná se o distribuci Java SDK a dále java konektor pro databázi MySQL. Konečná adresářová struktura v souborovém systém hostitelského počítače by tedy měla vypadat následovně:

----
/yum
    /repo
        /ambari <1>
        /hdp    
            /HDP         <2>      
            /HDP-UTILS   <3>             
        /common
         |- jdk-8u231-linux-x64.tar.gz  <4>   
         |- mysql-connector-java.jar    <5> 
----  
<1> Obsah Yum repozitáře Ambari. Viz <<hdp-yum-repo>>
<2> Obsah Yum repozitáře HDP. Viz <<hdp-yum-repo>>
<3> Obsah Yum repozitáře HDP-UTILS. Viz <<hdp-yum-repo>>
<4> Distribuce Oracle Java 8 SDK (build 231).
<5> Java konkektor pro databázi MySQL.

Klastr se bude složen ze čtyř virtuálních strojů viz. <<cluster-topolgy>>. Stroj c7201 bude obsahovat všechny řídící procesy pro HDFS a Yarn. Dále bude obsahovat utility pro volání služeb klusteru. V produkčním nasazení jsou typicky tyto dvě role rozděleny na rozdílných uzlech. Dále bude klastr obsahovat tři naprosto identické uzly c7202, c7203, c7204. Tyto uzly budou zpracovávat jednotlivé ulohy distribuovaných výpočtů. Každý z těchto uzlů zároveň slouži jako datový uzel pro HDFS. Počet pracovních uzlů (3) odpovídá výchozímu replikačnímu faktoru HDFS. Reálně to tedy znamená že jsou všechny data uloženy na všech třech pracovních uzech.          

[[cluster-topolgy]]
[title="Topologie klastru", options="header"]
|=========================================================
|Jméno |IP adresa |RAM   |Počet jader| Role
|c7201|192.168.72.101|8GB|3|Master, Edge
|c7202|192.168.72.102|5GB|2|Worker
|c7203|192.168.72.103|5GB|2|Worker
|c7204|192.168.72.104|5GB|2|Worker
|=========================================================

Soubor Vagrantfile je uložen v podadresáři vagrant v kořenovém adresáři projektu. V tomto souboru jsou nadefinovány jednotlivé virtuální stroje a jejich parametry. 

[source, ruby, numbered]
----
include::../../../vagrant/Vagrantfile[tags=box-def]
----

Atribut config.vm.box obsahuje definici boxu z kterého budou vytvořeny všechny nadefinované virtuální stroje. Tyto boxy jsou uloženy ve veřejném repositáři projektu Vagrant. Pŕi vztvoření prvního virtuálního stroje se tento box automaticky stáhne z příslušného repozitáře a následně se používá lokálně uložena kopie.

[source, ruby, numbered]
----
include::../../../vagrant/Vagrantfile[tags=synced-folders]
----

Atributy config.vm.synced_folder obsahují definice sdílených adresářů. Dle této definice se adresáře ze souborového systému automaticky připojí na souborový systém virtuálních strojů. První definice připoji kořenový adresář projektu do adresáře /opt/dev virtuálních strojů. Tato definice požívá relativní cestu vzhledem k Vagrantfile souboru. Druhá definice pripojí lokalní Yum repozitař do adresáře /var/www/html virtuálních strojů. Soubory uložené v tomto adresáři jsou následně dostupné přes HTTP protokol a příslušné URL mohou být použité k definici jednotlivých Yum repozitářů na  virtuálních strojích.
 
[source, ruby, numbered]
----
include::../../../vagrant/Vagrantfile[tags=vm-settings]
----

Sekce config.vm.provider obsahuje konfigurace specifické pro virtualizační platformu. V tomto případě instruujeme VirtualBox kolik operační paměti a jader procesoru má dát k dispozici jednotlivým virtuálním strojům.  

[source, ruby, numbered]
----
include::../../../vagrant/Vagrantfile[tags=c7202]             
----

Sekce config.vm.define obsahuje definici virtuálního stroje c7202 viz <<cluster-topolgy>>. Pro tento virtální stroj defujeme pouze specifickou IP adresu a jméno hosta. Všechny ostatní paramtetry jsou nadefinovány globálně, společně pro všechny virtuálním stroje. Definice ostatních strojů je analogická k tomuto nastavení. Incializaci klastru spustíme z příkazové řádky relativne k deskriptou Vagrantfile:

----
  > vagrant up
----