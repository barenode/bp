Pro test byl vybran volne dostuppny http://www2.informatik.uni-freiburg.de/~cziegler/BX/[Book-Crossing Dataset]. Jedna se o data z celosvetove databaze knih https://www.bookcrossing.com[Bookcrossing]. Dataset je slozeny s nasledujicich CSV souboru:

::BX-Users
    Soubor obsahuje seznam uzivatelu. Tento seznam byl anonymizovan, tj. kazdy uzivatel je oznacen pouze celociselnym identifikatorem. V neterych pripadech jsou poskytnuta dodatecna demograficka data o veku a bydlisti.

::BX-Books
    Knihy jsou identifikovany pomoci prislusneho ISBN. Soubor obsahuje dodatecne informace jako jsou jmeno titulu, jmeno autora, datum vydani a vydavatele. K nekterym titulum je uveden odkaz na ilustraci z titulni stranky.

::BX-Book-Ratings
    Tento soubor obsahuje hodnoceni. Tyt hodnoceni jsou bud explicitni, vyjadrene na stupnici od 1-10, nebo implicitni, tyto maji prirazenou hodnotu 0. 
    
Pred vlastni analyzou je nutne ulozit prislusna data do HDFS, sdileneho souboroveho systemu. K praci s HDFS existuje radkova utilita hdfs. Nejprve tedy vytvorime adresarovou strukturu a nasledne do techto adresaru nakopirujeme prislusne soubory z lokalniho souboroveho systemu.

----
hdfs dfs -mkdir /data
hdfs dfs -mkdir /data/books
hdfs dfs -copyFromLocal /opt/dev/src/main/jupyter/data/BX-Book-Ratings.csv /data/books
hdfs dfs -copyFromLocal /opt/dev/src/main/jupyter/data/BX-Books.csv /data/books
hdfs dfs -copyFromLocal /opt/dev/src/main/jupyter/data/BX-Users.csv /data/books
----
 
[source, ipython3]
----
%env SPARK_HOME=/usr/hdp/current/spark2-client
import findspark
findspark.init()
import pyspark
from pyspark.sql import SparkSession
spark = SparkSession.builder.getOrCreate()
print('Spark je pripraven ...')
----


----
env: SPARK_HOME=/usr/hdp/current/spark2-client
Spark je pripraven ...
----

[source, ipython3]
----
Nejprve zjistime zakladni informace o soboru kde jsou ulozena jednotliva hodnoceni. 
----

[source, ipython3]
----
ratings = spark.read.format("csv")\
    .option("sep",";")\
    .option("header", "true")\
    .load("/data/books/BX-Book-Ratings.csv")

ratings.printSchema()

print("Pocet hodnoceni = %i" % ratings.count())
----



    ---------------------------------------------------------------------------

    Py4JJavaError                             Traceback (most recent call last)

    /usr/hdp/current/spark2-client/python/pyspark/sql/utils.py in deco(*a, **kw)
         62         try:
    ---> 63             return f(*a, **kw)
         64         except py4j.protocol.Py4JJavaError as e:


    /usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
        327                     "An error occurred while calling {0}{1}{2}.\n".
    --> 328                     format(target_id, ".", name), value)
        329             else:


    Py4JJavaError: An error occurred while calling o61.load.
    : org.apache.spark.sql.AnalysisException: Path does not exist: hdfs://c7201.barenode.org:8020/data/books/BX-Book-Ratings.csv;
    	at org.apache.spark.sql.execution.datasources.DataSource$.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:719)
    	at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$15.apply(DataSource.scala:390)
    	at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$15.apply(DataSource.scala:390)
    	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
    	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
    	at scala.collection.immutable.List.foreach(List.scala:392)
    	at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)
    	at scala.collection.immutable.List.flatMap(List.scala:355)
    	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:389)
    	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:239)
    	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:227)
    	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:174)
    	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    	at java.lang.reflect.Method.invoke(Method.java:497)
    	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
    	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
    	at py4j.Gateway.invoke(Gateway.java:282)
    	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
    	at py4j.commands.CallCommand.execute(CallCommand.java:79)
    	at py4j.GatewayConnection.run(GatewayConnection.java:238)
    	at java.lang.Thread.run(Thread.java:745)


    
    During handling of the above exception, another exception occurred:


    AnalysisException                         Traceback (most recent call last)

    <ipython-input-2-e33a9135c3f9> in <module>
          2     .option("sep",";")\
          3     .option("header", "true")\
    ----> 4     .load("/data/books/BX-Book-Ratings.csv")
          5 
          6 ratings.printSchema()


    /usr/hdp/current/spark2-client/python/pyspark/sql/readwriter.py in load(self, path, format, schema, **options)
        164         self.options(**options)
        165         if isinstance(path, basestring):
    --> 166             return self._df(self._jreader.load(path))
        167         elif path is not None:
        168             if type(path) != list:


    /usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py in __call__(self, *args)
       1255         answer = self.gateway_client.send_command(command)
       1256         return_value = get_return_value(
    -> 1257             answer, self.gateway_client, self.target_id, self.name)
       1258 
       1259         for temp_arg in temp_args:


    /usr/hdp/current/spark2-client/python/pyspark/sql/utils.py in deco(*a, **kw)
         67                                              e.java_exception.getStackTrace()))
         68             if s.startswith('org.apache.spark.sql.AnalysisException: '):
    ---> 69                 raise AnalysisException(s.split(': ', 1)[1], stackTrace)
         70             if s.startswith('org.apache.spark.sql.catalyst.analysis'):
         71                 raise AnalysisException(s.split(': ', 1)[1], stackTrace)


    AnalysisException: 'Path does not exist: hdfs://c7201.barenode.org:8020/data/books/BX-Book-Ratings.csv;'

Nejprve jsme nacetli dany soubor do DataFrane struktury. Dataset ma tri sloupce User-ID, ISBN a Book-Rating. Prvni dva identifikuji uzivatele resp. knihu. Sloupec Book-Rating obsahuje prislusne hodnoceni. Ve vychozim stavu jsou vsechnz sloupce intepretovane jako retezcove promenne. Doporucovaci algoritmy nicmene vyzaduji aby hodnoceni byla numericka hodnota. Na datasetu tedy provedeme dodatecnou transaformaci, ktera vytvori dalsi slupec obsahujici numerickou repreyentaci slopuce Book-Rating.