
== Vytvoření modelu 

=== Získání dat   

Pro test algoritmu byl vybrán implicitní dataset volně dostupný přes API společnosti Last.fm. Tato společnost provozuje hudební web, založený ve Velké Británii v roce 2002. Pomocí systému hudebních doporučení s názvem „Audioscrobbler“ vytváří Last.fm podrobný profil hudebního vkusu každého uživatele zaznamenáním podrobností o skladbách, které uživatel poslouchá, a to buď z Internetové rozhlasové stanice, počítače uživatele nebo pomoci přenosneho hudebního zařízení. Tyto informace jsou přenášeny do databáze Last.fm buď prostřednictvím samotného hudebního přehrávače (mimo jiné včetně Spotify, Deezer, Tidal a MusicBee) nebo prostřednictvím plug-in nainstalovaného do hudebního přehrávače uživatele. Data se poté zobrazí na stránce profilu uživatele.

=== Příprava dat

Dataset je distribuován v archivu obsahujícím několik souborů. Relevantní data jsou uložena v jediném textovém souboru usersha1-artmbid-artname-plays.tsv. Data nejprve zkopírujeme z lokálního souborového systému do HDFS. V dalším kroku namapujeme soubor do datasetu, pomocí metody *show* zobrazíme první dva řádky :

[source, python, numbered]
---- 
rawData = spark.read.format('text').load("usersha1-artmbid-artname-plays.tsv")
rawData.show(2, False)
---- 

Dataset obsahuje jediný sloupec nazvaný *value*, obsah buněk odpovídá jednotlivým řádkům ze zdrojového souboru. Každý řádek obsahuje čtyři údaje. Identifikátor uživatele, identifikátor interpreta, název interpreta a počet přehrání některé skladby tohoto interpreta:
 
[%autofit]
----
+---------------------------------------------------------------------------------------------------------+
|value                                                                                                    |
+---------------------------------------------------------------------------------------------------------+
|00000c289a1829a808ac09c00daf10bc3c4e223b	3bd73256-3905-4f3a-97e2-8b341527f805	betty blowtorch	2137  |
|00000c289a1829a808ac09c00daf10bc3c4e223b	f2fb0ff0-5679-42ec-a55c-15109ce6e320	die Ärzte	1099      |
+---------------------------------------------------------------------------------------------------------+
----

Dalším krokem je převést jednotlivé řádky do struktury vhodnější pro další výpočty. Jednou z možností by bylo nadefinovat vlastní funkci, krerá by rozdělila vstpní řádek dle tabulátoru. Jednodušší postup ale je interpretovat vstupní soubor jako CSV formát s tabulátorem jako separátorem buněk. Spark obsahuje zabudovanou podporu pro tento formát spolu s řadou řídících voleb. V našem případě použijeme volbu *mode* s hodnotou DROPMALFORMED tak, že se při zpracování ignorují řádky, které nemají vyhovující formát. Dále aktivujeme volbu *inferSchema* aby se Spark pokusil přiřadit vhodné datové typy jednotlivým sloupcům. Nakonec přiřadíme sloupcům vhodná jména pomocí metody *withColumnRenamed*:  

[source, python, numbered]
---- 
df = spark.read.format('csv')\
    .option("header", "false")\
    .option("sep", "\t")\
    .option("mode", "DROPMALFORMED")\
    .option("inferSchema", "true")\
    .load("usersha1-artmbid-artname-plays.tsv")\
    .withColumnRenamed("_c0", "userHash")\
    .withColumnRenamed("_c1", "artistMBID")\
    .withColumnRenamed("_c2", "artistName")\
    .withColumnRenamed("_c3", "listenCount")
---- 

Pokud si nyní necháme vypsat schéma výsledného datasetu spolu s prvními třemi řádky:

[source, python, numbered]
----
df.printSchema()
df.show(3)
----

Dataset obsahuje čtyři sloupce s tím, že sloupec *listenCount* má přiřazený numerický datový typ: 

[%autofit]
----
root
 |-- userHash: string (nullable = true)
 |-- artistMBID: string (nullable = true)
 |-- artistName: string (nullable = true)
 |-- listenCount: double (nullable = true)

+--------------------+--------------------+-----------------+-----------+
|            userHash|          artistMBID|       artistName|listenCount|
+--------------------+--------------------+-----------------+-----------+
|00000c289a1829a80...|3bd73256-3905-4f3...|  betty blowtorch|     2137.0|
|00000c289a1829a80...|f2fb0ff0-5679-42e...|        die Ärzte|     1099.0|
|00000c289a1829a80...|b3ae82c2-e60b-455...|melissa etheridge|      897.0|
+--------------------+--------------------+-----------------+-----------+
only showing top 3 rows
----

Dalším problémem jsou datové typy sloupců userHash a artistName. ALS algoritmus  vyžaduje aby identikátory uživatelů resp. produktů byla celá čísla. Tyto jsou ale ve zdrojovém datasetu identifikovány řetězci, které se nedají automaticky převést na numerické hodnoty. Ze zdrojového datasetu tedy vytvoříme nový dataset obsahující pouze unikátní identifikátory uživatelů. Každému uživateli přiřadíme pomocí funkce *zipWithIndex* unikátní celočíselný identifikátor odpovádající indexu příslušného řádku v datasetu:

[source, python, numbered]
---- 
users = df.select(df.userHash).distinct()
users = users.rdd.zipWithIndex().toDF()
users = users.select(\
    users._1.userHash.alias("userHash"),\
    users._2.alias("userId").cast("integer")) 
---- 

Analogickou operaci jako s uživateli opakujeme i s inteprety a vytvoříme další dataset artists obsahující unikátní inteprety spolu s jejich celočíselnými identifikátory. Tyto dva datasety propojíme se zdrojovým datasetem a každému hodnocení přiřadíme celočíselné identifikátory uživatele a intepreta:

[source, python, numbered]
----
df = df\
    .join(users, df.userHash==users.userHash, 'inner')\
    .join(artists, df.artistName==artists.artistName, 'inner')\
    .select(users.userId, artists.artistId, df.listenCount.cast("float"))
----

Pokud necháme Spark vypsat schéma výsledného datasetu, oveříme, že se výsledný dataset ratings skládá ze tří sloupců. Sloupec userId pro celočíslený identifikátor uživatele, artistId pro celočíslený identifikátor interpreta a listenCount obsahující reálné číslo odpovídající počtu poslechů: 

[%autofit]
----
root
 |-- userId: integer (nullable = true)
 |-- artistId: integer (nullable = true)
 |-- listenCount: float (nullable = true)
----

=== Průzkum dat

Další fází je získat povědomí o datech ze kterých se následně pokusíme vytrénovat příslušný model. Nejprve zjistíme krajní hodnoty pro počet poslechů:

[source, python, numbered]
----
df.select(min("listenCount"), max("listenCount"), avg("listenCount")).show()
----

[%autofit]
----
+----------------+----------------+------------------+
|min(listenCount)|max(listenCount)|  avg(listenCount)|
+----------------+----------------+------------------+
|             1.0|        419157.0|215.18530888924704|
+----------------+----------------+------------------+
----

Z výstupu je patrné, že hodnoty nejsou v daatasetu rovnoměrně distribuované. Manimální hodnota, kdy je pro jediného uživatele 400 tisíc poslechů jediného intepreta, mnohonásobně převyšuje průměrnou hodnotu 215. Z <<ratings-dist>> je zřejmé, že se většina hodnot pohybuje kolem průměru ale dataset obsahuje malý počet extrémních hodnot.  

[[ratings-dist]]
image::../eda_files/eda_11_0.png[title="Rozložení hodnot v datasetu hodnocení", pdfwidth="100%"]  

=== Vytvoření tréningového a testovacího datasetu

Před vytvořením samotného modelu je třeba vyčlenit část dat tak, aby nebyla zahrnuta v tréningové fázi. Tento takzvaný testovací dataset bude použit pro vyhodnocení přesnosti modelu <<test-and-validation>>. Zdrojový dataset rozdělíme do dvou částí v poměru 70% pro trénovací dataset a zbylých 30% pro dataset testovací: 

[source, python, numbered]
----
train, test = ratings.randomSplit([0.7, 0.3])
----


=== Vytvoření iniciálního modelu

Iniciální model bude obsahovat výchozí hodnoty pro všechny parametry algoritmu tak jak jsou uvedeny <<als-estimator>>. Model bude vytvořen pouze na základě tréningových dat:

[source, python, numbered]
----
from mlonspark.alternating_least_square import AlternatingLeastSquare
alg = AlternatingLeastSquare()\
    .setUserCol("userId")\
    .setItemCol("artistId")\
    .setRatingCol("listenCount")

model = alg.fit(train)
----

Po vytvoření modelu vyhodnotíme jeho přesnost. Nejprve model pomocí metody *transform* aplikujeme na testovací i trénovací data. Tato operace do testovacího a trénovacího datasetu přidá nový sloupec prediction, který obsahuje predikci počtu poslechů:

[source, python, numbered]
----
trainPredictions = model.transform(train)
testPredictions = model.transform(test)
----

Následně pomocí vestavěné Spark třídy RegressionEvaluator spočítáme RMSE na základě naměřených poslechů ve sloupci listenCount a těch predikovaných ve sloupci prediction:

[source, python, numbered]
----
evaluator = RegressionEvaluator()\
    .setMetricName("rmse")\
    .setLabelCol("listenCount")\
    .setPredictionCol("prediction")
    
trainRmse = evaluator.evaluate(trainPredictions)
testRmse = evaluator.evaluate(testPredictions)
----

Výsledek jak pro trénovací RMSE tak i pro testovací obsahuje extrémně výsoké hodnoty. Průměrná chyba odpovídá trojnásobku průměru počtu poslechů. Také RMSE pro testovací data, která nebyla součástí učícího procesu, je paradoxně nižší než pro data trénovací:

----
train RMSE = 655.374786
test RMSE = 641.610511
----

Tento, extrémně nepřesný výsledek, je pravděpodobně způsobený nerovnoměrnou distribucí hodnot ve zdrojovém datasetu viz. <<ratings-dist>>. Zejména malý počet extrémních hodnot bude mít s velkou pravděpodobností za následek pokřivení výsledného modelu.


=== Úprava vstupních dat

Pro úpravu vstupních dat zvolíme jednoduchou metodu kdy z datasetu odstraníme nejnižších a nejvyšších 5% hodnot. Nejprve spočítáme 5% resp. 95% kvantil ze zdrojových dat:

[source, python, numbered]
----
from pyspark.sql import DataFrameStatFunctions as statFunc
quantiles = statFunc(df).approxQuantile("listenCount", [0.05, 0.95], 0.001)
---- 

----
[6.0, 751.0]
----

Z výsledku je patrné, že zejména hodnota 95% kvantilu 751 je výrazně nižší než maximální hodnota 419157. Po odstranění nejnižších a nejvyšších 5% hodnot je distribuce hodnot v datasetu výrazně rovnoměrnější viz. <<ratings-dist-filtered>>.

[[ratings-dist-filtered]]
image::../eda_files/eda_14_0.png[title="Rozložení hodnot v upraveném datasetu", pdfwidth="100%"]  

Pro model vytvořený na základě upraveného datasetu jsme naměřili RMSE 212.78 pro trénovací datset, resp. 213.15 pro dataset testovací. Tyto hodnoty se již blíží průměrné hodnotě 149.60. Také RMSE pro testovací dataset je vyšší než pro dataset trénovací.  

Další úpravou zdrojových dat je standardizace hodnot <<eda>>. Tato úprava převede hodnoty tak aby měly nulový průmer a směrodatnou odchylku rovnou 1. Problém je, že ALS algoritmus očekává pouze kladné hodnoty, proto všechny hodnoty posuneme tak, aby minimální hodnota byla rovná nule. Pro model vytvořený na základě standardizovaného datasetu jsme naměřili RMSE 1.297 pro trénovací datset, resp. 1.310 pro dataset testovací. 


=== Ladění hyper parametrů

V další fázi se pokusíme nalézt optimální hodnoty pro jednotlivé hyper parametry <<als-estimator>>. Optimální postup by byl použít křížovou validaci spolu se Spark třídou *ParamGridBuilder*, která umožní nalézt optimální hodnoty pro kombinace více hyper parametrů. Bohužel tato technika dalece přesahuje dostupný výkon použitého testovacího clustru. V našem případě budeme hledat optimální hodnotu pro každý hyper parametr zvlášť pro trénovací i testovací dataset. 

==== Rank

Z <<hp-rank>> je patrné, že RMSE klesá jak pro trénovací i testovací sadu pro rank menší než 25. Dále již klesá RMSE pouze pro sadu trénovací, což znamená že se model stává postupně přeučeným na tuto sadu. Pro další vzpočty použijeme hodnotu hyper parametru rank 25.  

[[hp-rank]]
image::../rank-tuning-stdpos_files/rank-tuning-stdpos_5_10.png[title="Rank", pdfwidth="60%"]  


==== Regulariční parametr

Z <<hp-regParam>> je patrné, že i minimální regularizace má negativní vliv na výsledný model. Typicky by se měla regularizace aplikovat pokud by docházelo k přeučení modelu s velkým rozdílem mezi trénovací a testovací sadou. V našem případě se obejdeme bez regularizace a použijeme nulový regulariční parametr. 

[[hp-regParam]]
image::../regParam-tuning-stdpos_files/regParam-tuning-stdpos_5_10.png[title="Regulariční parametr", pdfwidth="60%"]  

==== Alpha parametr

Z <<hp-alphaParam>> je patrné, že pro alpha parametr poskytují lepší výsledky vyšší hodnoty než výchozí hodnota 1. Pro další výpočty použijeme hodnotu hyper parametru alpha 10.        

[[hp-alphaParam]]
image::../alpha-tuning-stdpos_files/alpha-tuning-stdpos_7_1.png[title="Alpha parametr", pdfwidth="60%"]  


==== Počet iterací

Počet iterací by se měl nastavit dostatečně vysoký aby nadále nedocházelo ke snížení výsledného RMSE, tj. tato hodnota by měla uváznout ve svém minimu. Z <<hp-maxIter>> je zřejmé že výchozí počet iterací 10 je dostatečný, hodnota RMSE se ustálí již po šesté iteraci.

[[hp-maxIter]]
image::../maxIter-tuning-stdpos_files/maxIter-tuning-stdpos_7_1.png[title="Počet iterací", pdfwidth="60%"] 

==== Optimální hodnoty hyper parametrů [[param-optimal-values]]

V <<hp-optimal-values>> je zobrazen přehled hyper parametrů a nalezených optimálních hodnot. Vzhledem k použité technice není zaručené, že nedošlo k uváznutí na lokálním minimu a neexistuje kombinace parametrů, která by dosáhla lepšich výsledků. 

.Optimální hodnoty hyper parametrů
[[hp-optimal-values]]
|===
|Paramter|Hodnota

|Rank
|25

|Regulariční parametr
|0.0

|Alpha parametr
|10

|Počet iterací
|10

|===

