
== Experimentani faze

=== Ziskani dat


Pro test algoritmu byl vybrán implicitní dataset volně dostupný přes API společnosti Last.fm. Tato společnost provozuje hudební web, založený ve Velké Británii v roce 2002. Pomocí systému hudebních doporučení s názvem „Audioscrobbler“ vytváří Last.fm podrobný profil hudebního vkusu každého uživatele zaznamenáním podrobností o skladbách, které uživatel poslouchá, a to buď z Internetové rozhlasové stanice, počítače uživatele nebo pomoci přenosneho hudebního zařízení. Tyto informace jsou přenášeny do databáze Last.fm buď prostřednictvím samotného hudebního přehrávače (mimo jiné včetně Spotify, Deezer, Tidal a MusicBee) nebo prostřednictvím plug-in nainstalovaného do hudebního přehrávače uživatele. Data se poté zobrazí na stránce profilu uživatele.

=== Příprava dat

Dataset je distribuován v archivu obsahujícím několik souborů. Relevantní data jsou uložena v jediném textovém souboru usersha1-artmbid-artname-plays.tsv. Data nejprve zkopírujeme z lokálního souborového systému do HDFS. V dalším kroku namapujeme soubor do datasetu, pomocí metody *show* zobrazíme první dva řádky :

[source, python, numbered]
---- 
rawData = spark.read.format('text').load("usersha1-artmbid-artname-plays.tsv")
rawData.show(2, False)
---- 

Dataset obsahuje jediný sloupec nazvaný *value*, obsah buněk odpovídá jednotlivým řádkům ze zdrojového souboru. Každý řádek obsahuje čtyři údaje. Identifikátor uživatele, identifikátor interpreta, název interpreta a počet přehrání některé skladby tohoto interpreta:
 
[%autofit]
----
+---------------------------------------------------------------------------------------------------------+
|value                                                                                                    |
+---------------------------------------------------------------------------------------------------------+
|00000c289a1829a808ac09c00daf10bc3c4e223b	3bd73256-3905-4f3a-97e2-8b341527f805	betty blowtorch	2137  |
|00000c289a1829a808ac09c00daf10bc3c4e223b	f2fb0ff0-5679-42ec-a55c-15109ce6e320	die Ärzte	1099      |
+---------------------------------------------------------------------------------------------------------+
----

Dalším krokem je převést jednotlivé řádky do struktury vhodnější pro další výpočty. Jednou z možností by bylo nadefinovat vlastní funkci, krerá by rozdělila vstpní řádek dle tabulátoru. Jednodušší postup ale je interpretovat vstupní soubor jako CSV formát s tabulátorem jako separátorem buněk. Spark obsahuje zabudovabnou podporu pro tento formát spolu s řadou řídících voleb. V našem případě použijeme volbu *mode* s hodnotou DROPMALFORMED tak že se při zpracování ignorují řádky, které nemají vyhovující formát. Dále aktivujeme volbu *inferSchema* aby se Spark pokusil přiřadit vhodné datové typy jednotlivým sloupcům. Nakonec přiřadíme sloupcům vhodná jména pomocí metody *withColumnRenamed*:  

[source, python, numbered]
---- 
df = spark.read.format('csv')\
    .option("header", "false")\
    .option("sep", "\t")\
    .option("mode", "DROPMALFORMED")\
    .option("inferSchema", "true")\
    .load("usersha1-artmbid-artname-plays.tsv")\
    .withColumnRenamed("_c0", "userHash")\
    .withColumnRenamed("_c1", "artistMBID")\
    .withColumnRenamed("_c2", "artistName")\
    .withColumnRenamed("_c3", "listenCount")
---- 

Pokud si nyní necháme vypsat schéma výsledného datasetu spolu s prvními třemi řádky:

[source, python, numbered]
----
df.printSchema()
df.show(3)
----

Dataset obsahuje čtyři sloupce s tím, že sloupec *listenCount* má přiřazený numerický datový typ: 

[%autofit]
----
root
 |-- userHash: string (nullable = true)
 |-- artistMBID: string (nullable = true)
 |-- artistName: string (nullable = true)
 |-- listenCount: double (nullable = true)

+--------------------+--------------------+-----------------+-----------+
|            userHash|          artistMBID|       artistName|listenCount|
+--------------------+--------------------+-----------------+-----------+
|00000c289a1829a80...|3bd73256-3905-4f3...|  betty blowtorch|     2137.0|
|00000c289a1829a80...|f2fb0ff0-5679-42e...|        die Ärzte|     1099.0|
|00000c289a1829a80...|b3ae82c2-e60b-455...|melissa etheridge|      897.0|
+--------------------+--------------------+-----------------+-----------+
only showing top 3 rows
----

Dalším problémem jsou datové typy sloupců userHash a artistName. ALS algoritmus  vyžaduje aby identikátory uživatelů resp. produktů byla celá čísla. Tito jsou ale ve zdrojovém datasetu identifikováni řetězci, které se nedají automaticky převést na numerické hodnoty. Ze zdrojového datasetu tedy vytvoříme nový dataset obsahující pouze unikátní identifikátory uživatelů. Každému uživateli přiřadíme pomocí funkce *zipWithIndex* unikátní celočíselný identifikátor odpovádající indexu příslušného řádku v datasetu:

[source, python, numbered]
---- 
users = df.select(df.userHash).distinct()
users = users.rdd.zipWithIndex().toDF()
users = users.select(\
    users._1.userHash.alias("userHash"),\
    users._2.alias("userId").cast("integer")) 
---- 

Analogickou operaci jako s uživateli opakujeme i s inteprety a vytvoříme další dataset artists obsahující unikátní inteprety spolu s jejich celočíselnými identifikátory. Tyto dva datasety propojíme se zdorojovým datasetem a každému hodnocení přiřadíme celočíselné identifikátory uživatele a intepreta:

[source, python, numbered]
----
df = df\
    .join(users, df.userHash==users.userHash, 'inner')\
    .join(artists, df.artistName==artists.artistName, 'inner')\
    .select(users.userId, artists.artistId, df.listenCount.cast("float"))
----

Pokud necháme Spark vypsat schéma výsledného datasetu, oveříme, že se výsledný dataset ratings skládá ze tří sloupců. Sloupec userId pro celočíslený identifikátor uživatele, artistId pro celočíslený identifikátor interpreta a listenCount obsahující reálné číslo odpovídající počtu poslechů: 

[%autofit]
----
root
 |-- userId: integer (nullable = true)
 |-- artistId: integer (nullable = true)
 |-- listenCount: float (nullable = true)
----

=== Průzkum dat

Další fází je získat povědomí o datech ze kterých se následně pokusíme vytrénovat příslušný model. Nejprve jistíme krajní hodnoty pro počet poslechů:

[source, python, numbered]
----
df.select(min("listenCount"), max("listenCount"), avg("listenCount")).show()
----

[%autofit]
----
+----------------+----------------+------------------+
|min(listenCount)|max(listenCount)|  avg(listenCount)|
+----------------+----------------+------------------+
|             1.0|        419157.0|215.18530888924704|
+----------------+----------------+------------------+
----

Z výstupu je patrné, že hodnoty nejsou v daatasetu rovnoměrně distribuované. Manimální hodnota, kdy je pro jediného uživatele 400 tisíc poslechů jediného intepreta mnohonásobně převyšuje průměrnou hodnotu 215. Z <<ratings-dist>> je zřejmé, že se většina hodnot pohybuje kolem průměru ale dataset obsahuje malý počet extrémních hodnot.  

[[ratings-dist]]
image::../eda_files/eda_11_0.png[title="Rozložení hodnot v datsetu hodnocení", pdfwidth="100%"]  

=== Vytvoření tréningového a testovacího datasetu

Před vytvořením samotného modelu je třeba vyčlenit část dat tak, aby nebyla zahrnuta v tréningové fázi. Tento takzvaný testovací dataset bude použit pro vyhodnocení přesnosti modelu <<test-and-validation>>. Zdrojový dataset rozdělíme do dvou částí v poměru 70% pro trénovací dataset a zbylých 30% pro dataset testovací: 

[source, python, numbered]
----
train, test = ratings.randomSplit([0.7, 0.3])
----


=== Vytvoření iniciálního modelu

Iniciální model bude obsahovat výchozí hodnoty pro všechny parametry algoritmu tak jak jsou uvedeny <<als-estimator>>. Model bude vytvořen pouze na základě tréningových dat:

[source, python, numbered]
----
from mlonspark.alternating_least_square import AlternatingLeastSquare
alg = AlternatingLeastSquare()\
    .setUserCol("userId")\
    .setItemCol("artistId")\
    .setRatingCol("listenCount")

model = alg.fit(train)
----

Po vytvoření modelu vyhodnotíme jeho přesnost. Nejprve model pomocí metody *transform* aplikujeme na testovací i trénovací data. Tato operace do testovacího a trénovacího datasetu přidá nový sloupec *prediction*, který obsahuje predikci počtu poslechů:

[source, python, numbered]
----
trainPredictions = model.transform(train)
testPredictions = model.transform(test)
----

Následně pomocí vestavěné Spark třídy RegressionEvaluator spočítáme RMSE na základě naměřených poslechů ve sloupci *listenCount* a těch predikovaných ve sloupci *prediction*:

[source, python, numbered]
----
evaluator = RegressionEvaluator()\
    .setMetricName("rmse")\
    .setLabelCol("listenCount")\
    .setPredictionCol("prediction")
    
trainRmse = evaluator.evaluate(trainPredictions)
testRmse = evaluator.evaluate(testPredictions)
----

Výsledek jak pro trénovací RMSE tak i pro testovací obsahuje extrémně výsoké hodnoty. Průměrná chyba odpovídá trojnásobku průměru počtu poslechů. Také RMSE pro testovací data, které nebyly součástí učícího procesu, je nižší než pro data trénovací:

----
train RMSE = 655.374786
test RMSE = 641.610511
----

Tento, extrémně nepřesný výsledek, je pravděpodobně způsobený nerovnoměrnou distribucí hodnot ve zdrojovém datasetu viz. <<ratings-dist>>. Zejména malý počet extrémních hodnot bude mít za následek pokřivení výsledného modelu.


=== Úprava vstupních dat

Pro úpravu vstupnćích dat zvolíme jednoduchou metodu kdy z datasetu odstraníme nejnižších a nejvyšších 5% hodnot. Nejprve spočítáme 5% resp. 95% kvantil ze sdrojových dat:

  






