[source, ipython3]
----
%load_ext autoreload
%env SPARK_HOME=/usr/hdp/current/spark2-client

import findspark
findspark.init()
print('findspark initialized ...')

import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import expr, col, column, max, min
----


----
env: SPARK_HOME=/usr/hdp/current/spark2-client
findspark initialized ...
----

[source, ipython3]
----
def initspark():
    spark = SparkSession.builder.appName('mlonspark')\
        .config('spark.executor.instances', '3')\
        .getOrCreate()
    print('pyspark ready ...')
    return spark
----

[source, ipython3]
----
from pyspark.ml.recommendation import ALS
def initalg(val):
    return ALS()\
    .setUserCol("userId")\
    .setItemCol("artistId")\
    .setRatingCol("stdCountPos")\
    .setRank(val)\
    .setImplicitPrefs(True)

----

[source, ipython3]
----
import matplotlib as mpl
import matplotlib.pyplot as plt

def plot(res):
    x = []
    yTrain = []
    yTest = []
    for v in res:
        x.append(v[0])
        yTrain.append(v[1])
        yTest.append(v[2])

    plt.figure(figsize=(8,4))
    plt.plot(x, yTrain, "g-", linewidth=2, label=r"$TRAIN$")
    plt.plot(x, yTest, "r:", linewidth=2, label=r"TEST")
    plt.legend(loc="upper left", fontsize=15)
    plt.ylabel("RMSE", fontsize=18)
----

[source, ipython3]
----
from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.sql.functions import isnan

def runtest(rng, **model_kargs):
    res = []
    for i in rng:
        spark = initspark();
        train = spark.read.load("/data/lastfm-dataset-360K/data-filtered-std-pos-train.parquet")
        test = spark.read.load("/data/lastfm-dataset-360K/data-filtered-std-pos-test.parquet")
        
        evaluator = RegressionEvaluator()\
            .setMetricName("rmse")\
            .setLabelCol("stdCountPos")\
            .setPredictionCol("prediction")
        
        alg = initalg(i)
        model = alg.fit(train)
        trainPredictions = model.transform(train)
        trainPredictionsFiltered = trainPredictions.where(~isnan(col("prediction")))  
        trainRmse = evaluator.evaluate(trainPredictionsFiltered)
        
        
        testPredictions = model.transform(test)
        testPredictionsFiltered = testPredictions.where(~isnan(col("prediction")))    
        
        
        testRmse = evaluator.evaluate(testPredictionsFiltered)
        
        
        res.append([i, trainRmse, testRmse])
        plot(res)
        spark.stop()
    return res
----

[source, ipython3]
----
%matplotlib inline

import numpy as np
rng = np.arange(1, 90, 5).tolist()
res = runtest(rng)
print(res)
----


----
pyspark ready ...
pyspark ready ...
pyspark ready ...
pyspark ready ...
pyspark ready ...
pyspark ready ...
pyspark ready ...
pyspark ready ...
pyspark ready ...
pyspark ready ...
pyspark ready ...
pyspark ready ...
----


    ---------------------------------------------------------------------------

    Py4JJavaError                             Traceback (most recent call last)

    <ipython-input-6-b5a93a654784> in <module>
          3 import numpy as np
          4 rng = np.arange(1, 90, 5).tolist()
    ----> 5 res = runtest(rng)
          6 print(res)


    <ipython-input-5-c7bfb8b89e92> in runtest(rng, **model_kargs)
          6     for i in rng:
          7         spark = initspark();
    ----> 8         train = spark.read.load("/data/lastfm-dataset-360K/data-filtered-std-pos-train.parquet")
          9         test = spark.read.load("/data/lastfm-dataset-360K/data-filtered-std-pos-test.parquet")
         10 


    /usr/hdp/current/spark2-client/python/pyspark/sql/readwriter.py in load(self, path, format, schema, **options)
        164         self.options(**options)
        165         if isinstance(path, basestring):
    --> 166             return self._df(self._jreader.load(path))
        167         elif path is not None:
        168             if type(path) != list:


    /usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py in __call__(self, *args)
       1255         answer = self.gateway_client.send_command(command)
       1256         return_value = get_return_value(
    -> 1257             answer, self.gateway_client, self.target_id, self.name)
       1258 
       1259         for temp_arg in temp_args:


    /usr/hdp/current/spark2-client/python/pyspark/sql/utils.py in deco(*a, **kw)
         61     def deco(*a, **kw):
         62         try:
    ---> 63             return f(*a, **kw)
         64         except py4j.protocol.Py4JJavaError as e:
         65             s = e.java_exception.toString()


    /usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
        326                 raise Py4JJavaError(
        327                     "An error occurred while calling {0}{1}{2}.\n".
    --> 328                     format(target_id, ".", name), value)
        329             else:
        330                 raise Py4JError(


    Py4JJavaError: An error occurred while calling o2104.load.
    : org.apache.spark.SparkException: Job 0 cancelled because SparkContext was shut down
    	at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:837)
    	at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:835)
    	at scala.collection.mutable.HashSet.foreach(HashSet.scala:78)
    	at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:835)
    	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:1890)
    	at org.apache.spark.util.EventLoop.stop(EventLoop.scala:83)
    	at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1803)
    	at org.apache.spark.SparkContext$$anonfun$stop$8.apply$mcV$sp(SparkContext.scala:1936)
    	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1361)
    	at org.apache.spark.SparkContext.stop(SparkContext.scala:1935)
    	at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend$MonitorThread.run(YarnClientSchedulerBackend.scala:112)
    	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)
    	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2039)
    	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2060)
    	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2079)
    	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2104)
    	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)
    	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
    	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
    	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
    	at org.apache.spark.rdd.RDD.collect(RDD.scala:944)
    	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.mergeSchemasInParallel(ParquetFileFormat.scala:611)
    	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.inferSchema(ParquetFileFormat.scala:241)
    	at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$8.apply(DataSource.scala:203)
    	at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$8.apply(DataSource.scala:203)
    	at scala.Option.orElse(Option.scala:289)
    	at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:202)
    	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:393)
    	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:239)
    	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:227)
    	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:174)
    	at sun.reflect.GeneratedMethodAccessor72.invoke(Unknown Source)
    	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    	at java.lang.reflect.Method.invoke(Method.java:498)
    	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
    	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
    	at py4j.Gateway.invoke(Gateway.java:282)
    	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
    	at py4j.commands.CallCommand.execute(CallCommand.java:79)
    	at py4j.GatewayConnection.run(GatewayConnection.java:238)
    	at java.lang.Thread.run(Thread.java:748)




[[]]
image::../rank-tuning-stdpos_files/rank-tuning-stdpos_5_2.png[title=""]


[[]]
image::../rank-tuning-stdpos_files/rank-tuning-stdpos_5_3.png[title=""]


[[]]
image::../rank-tuning-stdpos_files/rank-tuning-stdpos_5_4.png[title=""]


[[]]
image::../rank-tuning-stdpos_files/rank-tuning-stdpos_5_5.png[title=""]


[[]]
image::../rank-tuning-stdpos_files/rank-tuning-stdpos_5_6.png[title=""]


[[]]
image::../rank-tuning-stdpos_files/rank-tuning-stdpos_5_7.png[title=""]


[[]]
image::../rank-tuning-stdpos_files/rank-tuning-stdpos_5_8.png[title=""]


[[]]
image::../rank-tuning-stdpos_files/rank-tuning-stdpos_5_9.png[title=""]


[[]]
image::../rank-tuning-stdpos_files/rank-tuning-stdpos_5_10.png[title=""]


[[]]
image::../rank-tuning-stdpos_files/rank-tuning-stdpos_5_11.png[title=""]


[[]]
image::../rank-tuning-stdpos_files/rank-tuning-stdpos_5_12.png[title=""]

[source, ipython3]
----
x = []
yTrain = []
yTest = []
for v in res:
    x.append(v[0])
    yTrain.append(v[1])
    yTest.append(v[2])

print(yTest)
----

[source, ipython3]
----
%matplotlib inline
import matplotlib as mpl
import matplotlib.pyplot as plt

plt.figure(figsize=(8,4))
plt.plot(x, yTrain, "g-", linewidth=2, label=r"$TRAIN$")
plt.plot(x, yTest, "r:", linewidth=2, label=r"TEST")
plt.legend(loc="upper left", fontsize=15)
plt.ylabel("RMSE", fontsize=18)
----

[source, ipython3]
----

----
